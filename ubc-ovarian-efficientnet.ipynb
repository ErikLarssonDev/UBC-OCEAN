{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":45867,"databundleVersionId":6924515,"sourceType":"competition"},{"sourceId":6799596,"sourceType":"datasetVersion","datasetId":3908722},{"sourceId":6866309,"sourceType":"datasetVersion","datasetId":3914448},{"sourceId":6957315,"sourceType":"datasetVersion","datasetId":3912057},{"sourceId":7074018,"sourceType":"datasetVersion","datasetId":3984980}],"dockerImageVersionId":30559,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment setup","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport math\nimport copy\nimport time\nimport random\nimport glob\nfrom typing import Dict, List\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom random import sample\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport torchvision\n\n# Utils\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score\n\n# For Image Models\nimport timm\nfrom PIL import Image\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom pathlib import Path\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\nos.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = str(pow(2,60))\nprint(f\"torch version {torch.__version__}\") \nprint(f'Torchvision version {torchvision.__version__}')","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:01.307892Z","iopub.execute_input":"2023-11-28T12:52:01.308290Z","iopub.status.idle":"2023-11-28T12:52:01.319991Z","shell.execute_reply.started":"2023-11-28T12:52:01.308255Z","shell.execute_reply":"2023-11-28T12:52:01.318988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\n    \"seed\": 42,\n    \"img_size\": 1200,\n    \"num_tiles\": 4,\n    \"model_name\": \"effnet-tiles-4-100e\",\n    \"num_classes\": 5,\n    \"valid_batch_size\": 16,\n    \"test_batch_size\": 1,\n    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    \"train\": False, # To train and save the model. Should be False when submitting\n    \"split_ratio\": 0.2,\n    \"num_workers\": os.cpu_count(),\n    \"epochs\": 50,\n    \"sandbox\": False, # True when finding optimal hyperparameters. Should be False when submitting.\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:01.321886Z","iopub.execute_input":"2023-11-28T12:52:01.322245Z","iopub.status.idle":"2023-11-28T12:52:01.334326Z","shell.execute_reply.started":"2023-11-28T12:52:01.322190Z","shell.execute_reply":"2023-11-28T12:52:01.333430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n# set_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:01.335564Z","iopub.execute_input":"2023-11-28T12:52:01.335854Z","iopub.status.idle":"2023-11-28T12:52:01.344982Z","shell.execute_reply.started":"2023-11-28T12:52:01.335830Z","shell.execute_reply":"2023-11-28T12:52:01.344252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR = '/kaggle/input/UBC-OCEAN'\nTEST_DIR = '/kaggle/input/UBC-OCEAN/test_thumbnails'\nALT_TEST_DIR = '/kaggle/input/UBC-OCEAN/test_images'\nTRAIN_DIR = '/kaggle/input/UBC-OCEAN/train_thumbnails'\nALT_TRAIN_DIR = '/kaggle/input/UBC-OCEAN/train_images'","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:01.346080Z","iopub.execute_input":"2023-11-28T12:52:01.346368Z","iopub.status.idle":"2023-11-28T12:52:01.357659Z","shell.execute_reply.started":"2023-11-28T12:52:01.346333Z","shell.execute_reply":"2023-11-28T12:52:01.356845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data processing","metadata":{}},{"cell_type":"code","source":"def get_test_file_path(image_id):\n    if os.path.exists(f\"{TEST_DIR}/{image_id}_thumbnail.png\"):\n        return f\"{TEST_DIR}/{image_id}_thumbnail.png\"\n    else: \n        return f\"{ALT_TEST_DIR}/{image_id}.png\"","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:01.360197Z","iopub.execute_input":"2023-11-28T12:52:01.360629Z","iopub.status.idle":"2023-11-28T12:52:01.367632Z","shell.execute_reply.started":"2023-11-28T12:52:01.360596Z","shell.execute_reply":"2023-11-28T12:52:01.366919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(f\"{ROOT_DIR}/test.csv\")\ndf['file_path'] = df['image_id'].apply(get_test_file_path)\ndf['label'] = 0 # dummy\ndf","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:01.368696Z","iopub.execute_input":"2023-11-28T12:52:01.369013Z","iopub.status.idle":"2023-11-28T12:52:01.388997Z","shell.execute_reply.started":"2023-11-28T12:52:01.368983Z","shell.execute_reply":"2023-11-28T12:52:01.388055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trim(im):\n    \"\"\"\n    Converts the image to grayscale using cv2, then computes binary matrix\n    of the pixels that are above a certrain threshold, then takes out the first\n    row where a certain percentage of the pixels are above the threshold will\n    be the first clip point. Same idea for col, max row, max col.\n    \"\"\"\n\n    percentage = 0.002\n    upper_percentage = 0.97\n\n    img = (np.array(im))\n    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    row_sums = np.sum(img_gray, axis=1)\n    col_sums = np.sum(img_gray, axis=0)\n    rows = np.where(np.logical_or(row_sums < img.shape[1] * percentage,\n                                  row_sums > img.shape[1] * upper_percentage))[0]\n    cols = np.where(np.logical_or(col_sums < img.shape[0] * percentage,\n                                  col_sums > img.shape[0] * upper_percentage))[0]\n    im_crop = np.delete(img, rows, axis=0)\n    im_crop = np.delete(im_crop, cols, axis=1)\n    return im_crop*255","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:01.390273Z","iopub.execute_input":"2023-11-28T12:52:01.390935Z","iopub.status.idle":"2023-11-28T12:52:01.398826Z","shell.execute_reply.started":"2023-11-28T12:52:01.390899Z","shell.execute_reply":"2023-11-28T12:52:01.397887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UBCDataset(Dataset):\n    def __init__(self, df, transforms=None):\n        self.df = df\n        self.file_names = df['file_path'].values\n        print(df['label'].values)\n        self.labels = df['label'].values\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path = self.file_names[index]\n        img = trim(plt.imread(img_path))\n        label = self.labels[index]\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n            \n        return torch.tensor(img), torch.tensor(label, dtype=torch.long)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:01.400028Z","iopub.execute_input":"2023-11-28T12:52:01.400332Z","iopub.status.idle":"2023-11-28T12:52:01.412253Z","shell.execute_reply.started":"2023-11-28T12:52:01.400306Z","shell.execute_reply":"2023-11-28T12:52:01.411379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_transforms = {\n    \"valid\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.),\n    \"train\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        # A.RandomBrightnessContrast(p=0.75),\n        A.ShiftScaleRotate(p=0.75),\n        A.OneOf([\n        A.GaussNoise(var_limit=[10, 50]),\n        A.GaussianBlur(),\n        A.MotionBlur(),\n        ], p=0.4),\n        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n        A.CoarseDropout(max_holes=1, max_width=int(512* 0.3), max_height=int(512* 0.3), mask_fill_value=0, p=0.5),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.)\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:01.522107Z","iopub.execute_input":"2023-11-28T12:52:01.523020Z","iopub.status.idle":"2023-11-28T12:52:01.532179Z","shell.execute_reply.started":"2023-11-28T12:52:01.522984Z","shell.execute_reply":"2023-11-28T12:52:01.531269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_file_path(image_id):\n    if os.path.exists(f\"{TRAIN_DIR}/{image_id}_thumbnail.png\"):\n        return f\"{TRAIN_DIR}/{image_id}_thumbnail.png\"\n    else:\n        return f\"{ALT_TRAIN_DIR}/{image_id}.png\"","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:01.534398Z","iopub.execute_input":"2023-11-28T12:52:01.534746Z","iopub.status.idle":"2023-11-28T12:52:01.544750Z","shell.execute_reply.started":"2023-11-28T12:52:01.534708Z","shell.execute_reply":"2023-11-28T12:52:01.543949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindf = pd.read_csv('/kaggle/input/UBC-OCEAN/train.csv')\ntraindf['file_path'] = traindf['image_id'].apply(get_train_file_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:01.545840Z","iopub.execute_input":"2023-11-28T12:52:01.546162Z","iopub.status.idle":"2023-11-28T12:52:01.957799Z","shell.execute_reply.started":"2023-11-28T12:52:01.546131Z","shell.execute_reply":"2023-11-28T12:52:01.956966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparing the training data frame for training","metadata":{}},{"cell_type":"code","source":"traindf['label'][traindf['label']==\"HGSC\"] = 0\ntraindf['label'][traindf['label']==\"EC\"] = 1\ntraindf['label'][traindf['label']==\"CC\"] = 2\ntraindf['label'][traindf['label']==\"LGSC\"] = 3\ntraindf['label'][traindf['label']==\"MC\"] = 4\ntraindf","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:01.958957Z","iopub.execute_input":"2023-11-28T12:52:01.959270Z","iopub.status.idle":"2023-11-28T12:52:01.982390Z","shell.execute_reply.started":"2023-11-28T12:52:01.959234Z","shell.execute_reply":"2023-11-28T12:52:01.981440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = UBCDataset(traindf, transforms=data_transforms[\"train\"]) # TODO: Add training transforms\ntrain_dataloader = DataLoader(train_dataset, batch_size=CONFIG['valid_batch_size'], \n                          num_workers=CONFIG[\"num_workers\"], shuffle=True, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:01.984876Z","iopub.execute_input":"2023-11-28T12:52:01.985171Z","iopub.status.idle":"2023-11-28T12:52:01.994162Z","shell.execute_reply.started":"2023-11-28T12:52:01.985147Z","shell.execute_reply":"2023-11-28T12:52:01.993346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_transformed_image(dataframe, transform):\n    # Select a random row from the DataFrame\n    random_row = dataframe.sample(n=1).iloc[0]\n\n    # Get the image file path from the selected row\n    file_path = random_row['file_path']\n    print(random_row['is_tma'])\n    print(file_path)\n    # Load the original image\n    original_image = plt.imread(file_path)\n    trimmed_image = trim(original_image)\n    \n    # Apply the transform to the original image\n    transformed_image = transform(image=trimmed_image)[\"image\"]\n\n    # Visualize the original and transformed images\n    fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n\n    axes[0].set_title('Original Image')\n    axes[0].imshow(original_image)\n    axes[0].axis('off')\n\n    axes[1].set_title('Transformed Image')\n    axes[1].imshow(transformed_image.permute(1, 2, 0))\n    axes[1].axis('off')\n\n    plt.show()\n\n# Visualize a random image from the DataFrame with the transform applied\nif CONFIG[\"sandbox\"]:\n    visualize_transformed_image(traindf, data_transforms[\"train\"])\n","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:01.995392Z","iopub.execute_input":"2023-11-28T12:52:01.995684Z","iopub.status.idle":"2023-11-28T12:52:02.005855Z","shell.execute_reply.started":"2023-11-28T12:52:01.995661Z","shell.execute_reply":"2023-11-28T12:52:02.005080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training and validation split from the training set","metadata":{}},{"cell_type":"code","source":"# Split the data into training and validation sets\nif CONFIG[\"sandbox\"]:\n    train_data, val_data = train_test_split(traindf, test_size=CONFIG[\"split_ratio\"], random_state=CONFIG[\"seed\"])\n\n    train_dataset = UBCDataset(train_data, transforms=data_transforms[\"train\"]) # TODO: Add training transforms\n    train_dataloader = DataLoader(train_dataset, batch_size=CONFIG['valid_batch_size'], \n                              num_workers=CONFIG[\"num_workers\"], shuffle=True, pin_memory=True)\n\n    val_dataset = UBCDataset(val_data, transforms=data_transforms[\"valid\"]) \n    val_dataloader = DataLoader(val_dataset, batch_size=CONFIG['valid_batch_size'], \n                              num_workers=CONFIG[\"num_workers\"], shuffle=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.006941Z","iopub.execute_input":"2023-11-28T12:52:02.007738Z","iopub.status.idle":"2023-11-28T12:52:02.020456Z","shell.execute_reply.started":"2023-11-28T12:52:02.007703Z","shell.execute_reply":"2023-11-28T12:52:02.019585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and save model","metadata":{}},{"cell_type":"markdown","source":"## Save model","metadata":{}},{"cell_type":"code","source":"def save_model(model: torch.nn.Module,\n               target_dir: str,\n               model_name: str):\n    \"\"\"Saves a PyTorch model to a target directory.\n    \n    Args:\n        model: A target PyTorch model to save.\n        target_dir: A directory for saving the model to.\n        model_name: A filename for the saved model. Should include either \".pth\" or \".pt\" as the file extension.\n        \n    Example usage:\n        save_model(model=model_0,\n        targer_dir=\"models\", \n        model_name=\"model_1\")\n    \"\"\"\n\n    # Create target directory\n    target_dir_path = Path(target_dir)\n    target_dir_path.mkdir(parents=True,\n                          exist_ok=True)\n\n    # Create model save path\n    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"),  \"model_name should end with '.pt' or '.pth'\"\n    model_save_path = target_dir_path / model_name\n\n    # Save the model state_dict()\n    print(f\"[INFO] Saving model to : {model_save_path}\")\n    torch.save(obj=model,\n               f=model_save_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.021579Z","iopub.execute_input":"2023-11-28T12:52:02.021869Z","iopub.status.idle":"2023-11-28T12:52:02.031033Z","shell.execute_reply.started":"2023-11-28T12:52:02.021845Z","shell.execute_reply":"2023-11-28T12:52:02.030136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing a pretrained model\nThis is done online before we turn off the internet access. ","metadata":{}},{"cell_type":"markdown","source":"#### Indentity module for removing last classifier layer","metadata":{}},{"cell_type":"code","source":"class Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n        \n    def forward(self, x):\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.032009Z","iopub.execute_input":"2023-11-28T12:52:02.032298Z","iopub.status.idle":"2023-11-28T12:52:02.044401Z","shell.execute_reply.started":"2023-11-28T12:52:02.032274Z","shell.execute_reply":"2023-11-28T12:52:02.043513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG[\"train\"] or CONFIG[\"sandbox\"]:\n    # 1. Get pretrained weights for ViT-base\n    pretrained_weights = torchvision.models.EfficientNet_B7_Weights.DEFAULT\n    #pretrained_vit_weights=torch.load('/kaggle/input/vit-weights/vit_b_16-c867db91.pth')\n    # 2. Setup a ViT model instance with pretrained weights\n    pretrained_model = torchvision.models.efficientnet_b7(weights=pretrained_weights).to(CONFIG[\"device\"])\n\n    # 3. Freeze the base parameters\n    for parameter in pretrained_model.parameters():\n        parameter.requires_grad = False\n        parameter.to(CONFIG[\"device\"])\n\n    # 4. Change the classifier head\n    # set_seed()\n    # print(pretrained_model)\n\n    pretrained_model.classifier = Identity()\n    pretrained_model\n\n    save_model(pretrained_model, # Needs to be saved as a dataset so that it works offline\n             \"/kaggle/working/\",\n             f\"{CONFIG['model_name']}-pretrained.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.045430Z","iopub.execute_input":"2023-11-28T12:52:02.045736Z","iopub.status.idle":"2023-11-28T12:52:02.054755Z","shell.execute_reply.started":"2023-11-28T12:52:02.045712Z","shell.execute_reply":"2023-11-28T12:52:02.053879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_images_into_tiles(image_batch, tile_size):\n    # Get the dimensions of the input image batch\n    batch_size, channels, input_height, input_width = image_batch.shape\n\n    \n    # Calculate the number of tiles in the height and width directions\n    num_tiles_height = input_height // tile_size\n    num_tiles_width = input_width // tile_size\n\n    # Initialize the output batch\n    output_batch = torch.zeros((batch_size, num_tiles_height * num_tiles_width, channels, tile_size, tile_size)).to(CONFIG[\"device\"])\n\n    # Split each image in the batch into tiles\n    for b in range(batch_size):\n        for i in range(num_tiles_height):\n            for j in range(num_tiles_width):\n                h_start = i * tile_size\n                h_end = (i + 1) * tile_size\n                w_start = j * tile_size\n                w_end = (j + 1) * tile_size\n                tile = image_batch[b, :, h_start:h_end, w_start:w_end]\n                output_batch[b, i * num_tiles_width + j] = tile\n\n    return torch.tensor(output_batch, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.055747Z","iopub.execute_input":"2023-11-28T12:52:02.056011Z","iopub.status.idle":"2023-11-28T12:52:02.069621Z","shell.execute_reply.started":"2023-11-28T12:52:02.055988Z","shell.execute_reply":"2023-11-28T12:52:02.068905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG[\"train\"] or CONFIG[\"sandbox\"]:\n    # Define the number of rows and columns in the grid\n    rows, cols = CONFIG[\"img_size\"] // 600, CONFIG[\"img_size\"] // 600\n\n    img = plt.imread(\"/kaggle/input/UBC-OCEAN/train_thumbnails/10077_thumbnail.png\")\n    img = data_transforms[\"valid\"](image=trim(img))[\"image\"]\n    plt.imshow(img.permute(1, 2, 0))\n    plt.show()\n\n    img_batch = img.unsqueeze(0)\n    print(img_batch.shape)\n    # Split the transformed image into tiles\n    images = split_images_into_tiles(img_batch, 600)\n    print(images.shape)\n    # Ensure you have enough tiles for the 2x2 grid\n\n    if len(images[0]) >= rows * cols:\n        for i, image in enumerate(images):\n            fig, axes = plt.subplots(rows, cols, figsize=(8, 8))\n            print(image.shape)\n            for i in range(rows):\n                for j in range(cols):\n                    tile_index = i * cols + j\n                    tile = image[tile_index].permute(1, 2, 0)\n                    print(tile.shape)\n                    axes[i, j].imshow(tile.cpu().numpy())\n                    axes[i, j].set_title(f\"Tile {tile_index + 1}\")\n                    axes[i, j].axis('off')\n            plt.show()\n    else:\n        print(f\"Not enough tiles to create a {rows}x{cols} grid.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.070659Z","iopub.execute_input":"2023-11-28T12:52:02.070924Z","iopub.status.idle":"2023-11-28T12:52:02.081934Z","shell.execute_reply.started":"2023-11-28T12:52:02.070900Z","shell.execute_reply":"2023-11-28T12:52:02.081023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM,self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n        \n    def gem(self, x, p=3, eps=1e-6):\n        return F.avg_pool1d(x.clamp(min=eps).pow(p), x.size(-1)).pow(1./p) # Changed to 1d since we only want to pool over the tiles dimension\n        \n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.082984Z","iopub.execute_input":"2023-11-28T12:52:02.083261Z","iopub.status.idle":"2023-11-28T12:52:02.095926Z","shell.execute_reply.started":"2023-11-28T12:52:02.083233Z","shell.execute_reply":"2023-11-28T12:52:02.095159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pooling = GeM()\ntest_vector = torch.randn((16, 25, 2560))\nprint(test_vector.permute(0, 2, 1).shape)\nprint(pooling(test_vector.permute(0, 2, 1)).squeeze(dim=-1).shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.100799Z","iopub.execute_input":"2023-11-28T12:52:02.101095Z","iopub.status.idle":"2023-11-28T12:52:02.125525Z","shell.execute_reply.started":"2023-11-28T12:52:02.101072Z","shell.execute_reply":"2023-11-28T12:52:02.124691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_vector = torch.randn((16, 25, 2560))\ntest_vector = test_vector.mean(dim=(-2))\nprint(test_vector.shape)\nprint(test_vector.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.126527Z","iopub.execute_input":"2023-11-28T12:52:02.126798Z","iopub.status.idle":"2023-11-28T12:52:02.139066Z","shell.execute_reply.started":"2023-11-28T12:52:02.126775Z","shell.execute_reply":"2023-11-28T12:52:02.138246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class tilingModel(nn.Module):\n    def __init__(self, backbone, n_tiles, tile_size, backbone_output_dim):\n        super().__init__()\n        self.backbone, self.n_tiles, self.tile_size = backbone.to(CONFIG[\"device\"]), n_tiles, tile_size\n        self.GeM_pooling = GeM()\n        self.head = nn.Sequential(\n        nn.Dropout(0.8),\n        nn.Linear(in_features=backbone_output_dim, out_features=1024),\n        nn.ReLU(),\n        nn.Dropout(0.8),\n        nn.Linear(in_features=1024, out_features=128),\n        nn.ReLU(),\n        nn.Dropout(0.8),\n        nn.Linear(in_features=128, out_features=CONFIG[\"num_classes\"]),\n    ).to(CONFIG[\"device\"])\n        \n    def forward(self, x): # TODO: Make this work for one batch at a time\n        images = split_images_into_tiles(x, self.tile_size) # Split x into tiles Result: [Batch_size, n_tiles, channels, width, height]\n        batch_features = torch.tensor([]).to(CONFIG[\"device\"])\n        \n        for tile in range(self.n_tiles): # TODO: Skip completely black tiles? Or maybe trim tiles?\n            tile_batch = images[:, tile, :, :, :] # Desired shape of [batch_size, channels, width, height]\n            batch_features = torch.cat((batch_features, self.backbone(tile_batch).unsqueeze(1)), dim = 1)\n        batch_features = batch_features.mean(dim=(-2)) # Average pooling\n        # batch_features = self.GeM_pooling(batch_features.permute(0, 2, 1)).squeeze(dim=-1)\n        x = self.head(batch_features)\n        return(x)\n\nif CONFIG[\"train\"] or CONFIG[\"sandbox\"]:                                                                        \n    tilingmodel = tilingModel(pretrained_model, CONFIG[\"num_tiles\"], 600, 2560).to(CONFIG[\"device\"])\n\n    img = plt.imread(\"/kaggle/input/UBC-OCEAN/train_thumbnails/10077_thumbnail.png\")\n    img = data_transforms[\"valid\"](image=img)[\"image\"]\n\n    img2 = plt.imread(\"/kaggle/input/UBC-OCEAN/train_thumbnails/10143_thumbnail.png\")\n    img2 = data_transforms[\"valid\"](image=img2)[\"image\"]\n\n    img_batch = torch.stack((img, img2))\n    print(f\"img_batch: {img_batch.shape}\")\n    output = tilingmodel(img_batch)\n    print(f\"Shape of output: {output.shape}\")\n    print(output)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.140337Z","iopub.execute_input":"2023-11-28T12:52:02.140588Z","iopub.status.idle":"2023-11-28T12:52:02.152359Z","shell.execute_reply.started":"2023-11-28T12:52:02.140565Z","shell.execute_reply":"2023-11-28T12:52:02.151454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"\"\"\"\nContains functions for training and testing a PyTorch model.\n\"\"\"\nimport torch\n\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -> Tuple[float, float]:\n    \"\"\"Trains a PyTorch model for a single epoch.\n\n    Turns a target PyTorch model to training mode and then\n    runs through all of the required training steps (forward\n    pass, loss calculation, optimizer step).\n\n    Args:\n    model: A PyTorch model to be trained.\n    dataloader: A DataLoader instance for the model to be trained on.\n    loss_fn: A PyTorch loss function to minimize.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n    Returns:\n    A tuple of training loss and training accuracy metrics.\n    In the form (train_loss, train_accuracy). For example:\n\n    (0.1112, 0.8743)\n    \"\"\"\n    # Put model in train mode\n    model.train()\n\n    # Setup train loss and train accuracy values\n    train_loss, train_acc = 0, 0\n\n    # Loop through data loader data batches\n    for batch, (X, y) in enumerate(dataloader):\n        # Send data to target device\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate  and accumulate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item() \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Calculate and accumulate accuracy metric across all batches\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n    # Adjust metrics to get average loss and accuracy per batch \n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -> Tuple[float, float]:\n    \"\"\"Tests a PyTorch model for a single epoch.\n\n    Turns a target PyTorch model to \"eval\" mode and then performs\n    a forward pass on a testing dataset.\n\n    Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n    Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n\n    (0.0223, 0.8985)\n    \"\"\"\n    # Put model in eval mode\n    model.eval() \n\n    # Setup test loss and test accuracy values\n    test_loss, test_acc, balanced_acc = 0, 0, 0\n    pred_labels = []\n    true_labels = []\n    # Turn on inference context manager\n    with torch.inference_mode():\n        # Loop through DataLoader batches\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to target device\n            X, y = X.to(device), y.to(device)\n\n            # 1. Forward pass\n            test_pred_logits = model(X)\n\n            # 2. Calculate and accumulate loss\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n\n            # Calculate and accumulate accuracy\n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n            pred_labels = pred_labels + test_pred_labels.cpu().tolist()\n            true_labels = true_labels + y.cpu().tolist()\n\n    # Adjust metrics to get average loss and accuracy per batch \n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    balanced_acc = balanced_accuracy_score(true_labels, pred_labels)\n    return test_loss, test_acc, balanced_acc\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -> Dict[str, List]:\n    \"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Args:\n    model: A PyTorch model to be trained and tested.\n    train_dataloader: A DataLoader instance for the model to be trained on.\n    test_dataloader: A DataLoader instance for the model to be tested on.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n    epochs: An integer indicating how many epochs to train for.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n    Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for \n    each epoch.\n    In the form: {train_loss: [...],\n                  train_acc: [...],\n                  test_loss: [...],\n                  test_acc: [...]} \n    For example if training for epochs=2: \n                 {train_loss: [2.0616, 1.0537],\n                  train_acc: [0.3945, 0.3945],\n                  test_loss: [1.2641, 1.5706],\n                  test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n                \"train_acc\": [],\n                \"test_loss\": [],\n                \"test_acc\": [],\n                \"balanced_acc\": [],\n                }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n        test_loss, test_acc, balanced_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n        # Print out what's happening\n        print(\n                f\"Epoch: {epoch+1} | \"\n                f\"train_loss: {train_loss:.4f} | \"\n                f\"train_acc: {train_acc:.4f} | \"\n                f\"test_loss: {test_loss:.4f} | \"\n                f\"test_acc: {test_acc:.4f} | \"\n                f\"balanced_acc: {balanced_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n        results[\"balanced_acc\"].append(balanced_acc)\n\n    # Return the filled results at the end of the epochs\n    return results","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.153669Z","iopub.execute_input":"2023-11-28T12:52:02.153959Z","iopub.status.idle":"2023-11-28T12:52:02.177713Z","shell.execute_reply.started":"2023-11-28T12:52:02.153936Z","shell.execute_reply":"2023-11-28T12:52:02.176777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create optimizer and loss function\ndef train_model(train_dl, test_dl, model_name, store_model=True):\n    # tilingmodel = tilingModel(pretrained_model, 4, 600, 2560).to(CONFIG[\"device\"])\n    tilingmodel = torch.load('/kaggle/input/effnet-th-tiles-25-trained/effnet-th-tiles-4_50e_sandbox.pth')\n    optimizer = torch.optim.Adam(params=tilingmodel.parameters(),\n                                 lr=1e-3)\n    loss_fn = torch.nn.CrossEntropyLoss()\n    \n    if test_dl is None:\n        test_dl = train_dl\n    # Train the classifier head of the pretrained ViT feature extractor model\n    # set_seed()\n    results = train(model=tilingmodel,\n                   train_dataloader=train_dl,\n                   test_dataloader=test_dl,\n                   optimizer=optimizer,\n                   loss_fn=loss_fn,\n                   epochs=CONFIG[\"epochs\"],\n                   device=CONFIG[\"device\"])\n    if store_model:\n        save_model(tilingmodel, # Needs to be saved as a dataset so that it works offline\n                 \"/kaggle/working/\",\n                 model_name)\n        \n    return results","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.178777Z","iopub.execute_input":"2023-11-28T12:52:02.179064Z","iopub.status.idle":"2023-11-28T12:52:02.191944Z","shell.execute_reply.started":"2023-11-28T12:52:02.179040Z","shell.execute_reply":"2023-11-28T12:52:02.191158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG[\"train\"]:\n    results = train_model(train_dl=train_dataloader, test_dl=train_dataloader, model_name=f\"{CONFIG['model_name']}.pth\")\nelif CONFIG[\"sandbox\"]:\n    results = train_model(train_dl=train_dataloader, test_dl=val_dataloader, model_name=f\"{CONFIG['model_name']}.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.192905Z","iopub.execute_input":"2023-11-28T12:52:02.193236Z","iopub.status.idle":"2023-11-28T12:52:02.205035Z","shell.execute_reply.started":"2023-11-28T12:52:02.193186Z","shell.execute_reply":"2023-11-28T12:52:02.204357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_loss_curves(results: Dict[str, List[float]]):\n    \"\"\"Plots training curves of a results dictionary.\n\n    Args:\n        results (dict): dictionary containing list of values, e.g.\n            {\"train_loss\": [...],\n             \"train_acc\": [...],\n             \"test_loss\": [...],\n             \"test_acc\": [...]}\n    \"\"\"\n\n    # Get the loss values of the results dictionary (training and test)\n    loss = results['train_loss']\n    test_loss = results['test_loss']\n\n    # Get the accuracy values of the results dictionary (training and test)\n    accuracy = results['train_acc']\n    test_accuracy = results['test_acc']\n\n    # Figure out how many epochs there were\n    epochs = range(len(results['train_loss']))\n\n    # Setup a plot\n    plt.figure(figsize=(15, 7))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label='train_loss')\n    plt.plot(epochs, test_loss, label='test_loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, label='train_accuracy')\n    plt.plot(epochs, test_accuracy, label='test_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend()\n    plt.show()\n    \n    # Plot accuracy\n    plt.subplot(1, 1, 1)\n    plt.plot(epochs, results[\"balanced_acc\"], label='balanced_acc')\n    plt.title('Balanced Accuracy for validation data')\n    plt.xlabel('Epochs')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.206014Z","iopub.execute_input":"2023-11-28T12:52:02.206324Z","iopub.status.idle":"2023-11-28T12:52:02.215670Z","shell.execute_reply.started":"2023-11-28T12:52:02.206300Z","shell.execute_reply":"2023-11-28T12:52:02.214897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the loss curves\nif CONFIG[\"train\"] or CONFIG[\"sandbox\"]:\n    plot_loss_curves(results) ","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.216680Z","iopub.execute_input":"2023-11-28T12:52:02.216966Z","iopub.status.idle":"2023-11-28T12:52:02.228421Z","shell.execute_reply.started":"2023-11-28T12:52:02.216942Z","shell.execute_reply":"2023-11-28T12:52:02.227648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing the model","metadata":{}},{"cell_type":"code","source":"if CONFIG[\"train\"] or CONFIG[\"sandbox\"]:\n    model = torch.load(f\"/kaggle/working/{CONFIG['model_name']}.pth\")\nelse:\n    # model = torch.load(f\"/kaggle/input/{CONFIG['model_name']}-trained/{CONFIG['model_name']}.pth\") # Needs to be loaded from the input folder so that it works offline\n    model = torch.load('/kaggle/input/effnet-th-tiles-25-trained/effnet-tiles-4-100e.pth')\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.229369Z","iopub.execute_input":"2023-11-28T12:52:02.229664Z","iopub.status.idle":"2023-11-28T12:52:02.610148Z","shell.execute_reply.started":"2023-11-28T12:52:02.229641Z","shell.execute_reply":"2023-11-28T12:52:02.609182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = UBCDataset(df, transforms=data_transforms[\"valid\"])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'], \n                          num_workers=CONFIG[\"num_workers\"], shuffle=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.611764Z","iopub.execute_input":"2023-11-28T12:52:02.612052Z","iopub.status.idle":"2023-11-28T12:52:02.617743Z","shell.execute_reply.started":"2023-11-28T12:52:02.612027Z","shell.execute_reply":"2023-11-28T12:52:02.616787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = [\"HGSC\",\"EC\",\"CC\",\"LGSC\",\"MC\"]\n\n# Function to load the model and predict on selected image\ndef predict_on_image(model, image_path, device=CONFIG[\"device\"]):\n    print(device)\n    # Load the image and turn it into torch.float32 (same type as model)\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    # Preprocess the image to get it between 0 and 1\n    transform = data_transforms[\"valid\"]\n    \n    image = transform(image=img)[\"image\"] # make sure image has batch dimension (shape: [batch_size, height, width, color_channels])\n    image = torch.tensor(np.expand_dims(image, axis=0))\n    print(image.to(device).dtype)\n    \n    # Predict on image\n    model.eval()\n    with torch.inference_mode():\n        # Put image to target device\n        image = image.to(device)\n        print(image.device)\n        \n        # Get prediction logits\n        pred_logits = model(image)\n        print(pred_logits)\n        # Get prediction probabilities\n        pred_probs = torch.softmax(pred_logits, dim=1)\n\n        # Get prediction label\n        pred_label = torch.argmax(pred_probs, dim=1).detach().cpu()\n        pred_label_class = class_names[pred_label]\n\n    print(f\"[INFO] Pred label: {pred_label} Pred class: {pred_label_class}, Pred prob: {pred_probs.max():.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.618972Z","iopub.execute_input":"2023-11-28T12:52:02.619344Z","iopub.status.idle":"2023-11-28T12:52:02.632285Z","shell.execute_reply.started":"2023-11-28T12:52:02.619318Z","shell.execute_reply":"2023-11-28T12:52:02.631452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG[\"device\"]\nclass_names[int(traindf['label'][traindf['image_id']==10077])]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.633345Z","iopub.execute_input":"2023-11-28T12:52:02.633616Z","iopub.status.idle":"2023-11-28T12:52:02.644437Z","shell.execute_reply.started":"2023-11-28T12:52:02.633592Z","shell.execute_reply":"2023-11-28T12:52:02.643596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG[\"train\"]:\n    predict_on_image(model, \"/kaggle/input/UBC-OCEAN/train_thumbnails/10077_thumbnail.png\", device=CONFIG[\"device\"])","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.645602Z","iopub.execute_input":"2023-11-28T12:52:02.645923Z","iopub.status.idle":"2023-11-28T12:52:02.653413Z","shell.execute_reply.started":"2023-11-28T12:52:02.645891Z","shell.execute_reply":"2023-11-28T12:52:02.652311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot images and make predictions for manual testing\ndef show_images_from_dataframe(dataframe, num_images):\n    # Select 'num_images' random rows from the DataFrame\n    selected_rows = dataframe.sample(num_images)\n    \n    for _, row in selected_rows.iterrows():\n        file_path = row['file_path']\n        image = cv2.imread(file_path)\n        \n        plt.figure(figsize=(30, 30))\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        plt.title(f\"Image ID: {row['image_id']}\")\n        plt.axis('off')\n        plt.show()\n    return selected_rows\n\nif CONFIG[\"sandbox\"]:        \n    selected_rows = show_images_from_dataframe(val_data, 5)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.654629Z","iopub.execute_input":"2023-11-28T12:52:02.654958Z","iopub.status.idle":"2023-11-28T12:52:02.663729Z","shell.execute_reply.started":"2023-11-28T12:52:02.654924Z","shell.execute_reply":"2023-11-28T12:52:02.662829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_and_visualize_images_with_labels(selected_rows, model):\n    model.eval()\n\n    for _, row in selected_rows.iterrows():\n        file_path = row['file_path']\n        image = cv2.imread(file_path)\n\n        # Apply the transform to the original image\n        transformed_image = data_transforms[\"valid\"](image=image)[\"image\"].unsqueeze(0).to(CONFIG[\"device\"])\n\n        # Perform inference\n        with torch.no_grad():\n            output = model(transformed_image)\n\n        # Get the predicted class label\n        predicted_class = torch.argmax(output).item()\n\n        plt.figure(figsize=(8, 8))\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        plt.title(f\"Image ID: {row['image_id']}\\nTrue Label: {class_names[row['label']]}\\nPredicted Label: {class_names[predicted_class]}\")\n        plt.axis('off')\n        plt.show()\n        \nif CONFIG[\"sandbox\"]:       \n    predict_and_visualize_images_with_labels(selected_rows, model)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.664833Z","iopub.execute_input":"2023-11-28T12:52:02.665378Z","iopub.status.idle":"2023-11-28T12:52:02.678062Z","shell.execute_reply.started":"2023-11-28T12:52:02.665350Z","shell.execute_reply":"2023-11-28T12:52:02.677211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_label(label):\n    return class_names[label]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.679280Z","iopub.execute_input":"2023-11-28T12:52:02.679549Z","iopub.status.idle":"2023-11-28T12:52:02.688274Z","shell.execute_reply.started":"2023-11-28T12:52:02.679527Z","shell.execute_reply":"2023-11-28T12:52:02.687352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nif CONFIG[\"sandbox\"]:\n    preds = []\n    true = []\n    with torch.no_grad():\n        bar = tqdm(enumerate(val_dataloader), total=len(val_dataloader))\n        for step, (data, y) in bar:        \n            images = data.to(CONFIG[\"device\"], dtype=torch.float)        \n            batch_size = images.size(0)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            true.append(y.detach().cpu().numpy())\n            preds.append(predicted.detach().cpu().numpy())\n    preds = [get_label(item) for item in np.concatenate(preds).flatten()]\n    true = [get_label(item) for item in np.concatenate(true).flatten()]\n\n    # Compute the confusion matrix\n    cm = confusion_matrix(true, preds)\n\n    # Create a ConfusionMatrixDisplay with display_labels parameter\n    cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n    cmd.plot(cmap=plt.cm.Blues)  # You can choose a different color map if needed\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.689311Z","iopub.execute_input":"2023-11-28T12:52:02.689571Z","iopub.status.idle":"2023-11-28T12:52:02.699648Z","shell.execute_reply.started":"2023-11-28T12:52:02.689549Z","shell.execute_reply":"2023-11-28T12:52:02.698691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create submission","metadata":{}},{"cell_type":"code","source":"preds = []\nwith torch.no_grad():\n    bar = tqdm(enumerate(test_loader), total=len(test_loader))\n    for step, (data, _) in bar:        \n        images = data.to(CONFIG[\"device\"], dtype=torch.float)        \n        batch_size = images.size(0)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        \n        preds.append( predicted.detach().cpu().numpy() )\npreds = np.concatenate(preds).flatten()\npreds\n","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:02.700656Z","iopub.execute_input":"2023-11-28T12:52:02.700973Z","iopub.status.idle":"2023-11-28T12:52:03.884736Z","shell.execute_reply.started":"2023-11-28T12:52:02.700941Z","shell.execute_reply":"2023-11-28T12:52:03.883716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub_2 = pd.read_csv(f\"{ROOT_DIR}/test.csv\")\ndf_sub_2","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:03.886169Z","iopub.execute_input":"2023-11-28T12:52:03.886553Z","iopub.status.idle":"2023-11-28T12:52:03.900371Z","shell.execute_reply.started":"2023-11-28T12:52:03.886515Z","shell.execute_reply":"2023-11-28T12:52:03.898861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub_2 = pd.read_csv(f\"{ROOT_DIR}/test.csv\")\ndf_sub_2[\"label\"] = preds    \ndf_sub_2[\"label\"] = df_sub_2[\"label\"].apply(get_label)\ndf_sub_2 = df_sub_2.drop(\"image_width\", axis=1)\ndf_sub_2 = df_sub_2.drop(\"image_height\", axis=1)\ndf_sub_2.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:03.901566Z","iopub.execute_input":"2023-11-28T12:52:03.901834Z","iopub.status.idle":"2023-11-28T12:52:03.912127Z","shell.execute_reply.started":"2023-11-28T12:52:03.901810Z","shell.execute_reply":"2023-11-28T12:52:03.911325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check that submission looks reasonable","metadata":{}},{"cell_type":"code","source":"df_sub_2","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:52:03.913256Z","iopub.execute_input":"2023-11-28T12:52:03.913606Z","iopub.status.idle":"2023-11-28T12:52:03.921652Z","shell.execute_reply.started":"2023-11-28T12:52:03.913564Z","shell.execute_reply":"2023-11-28T12:52:03.920755Z"},"trusted":true},"execution_count":null,"outputs":[]}]}