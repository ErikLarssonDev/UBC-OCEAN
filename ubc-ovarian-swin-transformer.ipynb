{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment setup","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport math\nimport copy\nimport time\nimport random\nimport glob\nfrom typing import Dict, List\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom random import sample\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport torchvision\n\n# Utils\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score\n\n# For Image Models\nimport timm\nfrom PIL import Image\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom pathlib import Path\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\nos.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = str(pow(2,60))\nprint(f\"torch version {torch.__version__}\") \nprint(f'Torchvision version {torchvision.__version__}')","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.318532Z","iopub.execute_input":"2023-11-02T17:30:30.318931Z","iopub.status.idle":"2023-11-02T17:30:30.331362Z","shell.execute_reply.started":"2023-11-02T17:30:30.318889Z","shell.execute_reply":"2023-11-02T17:30:30.330328Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"torch version 2.0.0\nTorchvision version 0.15.1\n","output_type":"stream"}]},{"cell_type":"code","source":"CONFIG = {\n    \"seed\": 42,\n    \"img_size\": 224,\n    \"model_name\": \"swin\",\n    \"num_classes\": 5,\n    \"valid_batch_size\": 64,\n    \"test_batch_size\": 1,\n    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    \"train\": False, # To train and save the model. Should be False when submitting\n    \"split_ratio\": 0.2,\n    \"num_workers\": os.cpu_count(),\n    \"epochs\": 50,\n    \"sandbox\": False, # True when finding optimal hyperparameters. Should be False when submitting.\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.333471Z","iopub.execute_input":"2023-11-02T17:30:30.333858Z","iopub.status.idle":"2023-11-02T17:30:30.346453Z","shell.execute_reply.started":"2023-11-02T17:30:30.333822Z","shell.execute_reply":"2023-11-02T17:30:30.345563Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.347675Z","iopub.execute_input":"2023-11-02T17:30:30.348073Z","iopub.status.idle":"2023-11-02T17:30:30.360007Z","shell.execute_reply.started":"2023-11-02T17:30:30.348037Z","shell.execute_reply":"2023-11-02T17:30:30.359157Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR = '/kaggle/input/UBC-OCEAN'\nTEST_DIR = '/kaggle/input/UBC-OCEAN/test_thumbnails'\nALT_TEST_DIR = '/kaggle/input/UBC-OCEAN/test_images'\nTRAIN_DIR = '/kaggle/input/UBC-OCEAN/train_thumbnails'\nALT_TRAIN_DIR = '/kaggle/input/UBC-OCEAN/train_images'","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.362223Z","iopub.execute_input":"2023-11-02T17:30:30.362540Z","iopub.status.idle":"2023-11-02T17:30:30.375312Z","shell.execute_reply.started":"2023-11-02T17:30:30.362515Z","shell.execute_reply":"2023-11-02T17:30:30.374392Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"# Data processing","metadata":{}},{"cell_type":"code","source":"def get_test_file_path(image_id):\n    if os.path.exists(f\"{TEST_DIR}/{image_id}_thumbnail.png\"):\n        return f\"{TEST_DIR}/{image_id}_thumbnail.png\"\n    else: \n        return f\"{ALT_TEST_DIR}/{image_id}.png\"","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.376570Z","iopub.execute_input":"2023-11-02T17:30:30.376872Z","iopub.status.idle":"2023-11-02T17:30:30.387827Z","shell.execute_reply.started":"2023-11-02T17:30:30.376846Z","shell.execute_reply":"2023-11-02T17:30:30.386866Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(f\"{ROOT_DIR}/test.csv\")\ndf['file_path'] = df['image_id'].apply(get_test_file_path)\ndf['label'] = 0 # dummy\ndf","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.388959Z","iopub.execute_input":"2023-11-02T17:30:30.389257Z","iopub.status.idle":"2023-11-02T17:30:30.415720Z","shell.execute_reply.started":"2023-11-02T17:30:30.389233Z","shell.execute_reply":"2023-11-02T17:30:30.414789Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"   image_id  image_width  image_height  \\\n0        41        28469         16987   \n\n                                           file_path  label  \n0  /kaggle/input/UBC-OCEAN/test_thumbnails/41_thu...      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>image_width</th>\n      <th>image_height</th>\n      <th>file_path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41</td>\n      <td>28469</td>\n      <td>16987</td>\n      <td>/kaggle/input/UBC-OCEAN/test_thumbnails/41_thu...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def trim(im):\n    \"\"\"\n    Converts the image to grayscale using cv2, then computes binary matrix\n    of the pixels that are above a certrain threshold, then takes out the first\n    row where a certain percentage of the pixels are above the threshold will\n    be the first clip point. Same idea for col, max row, max col.\n    \"\"\"\n\n    percentage = 0.002\n    upper_percentage = 0.97\n\n    img = (np.array(im))\n    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    row_sums = np.sum(img_gray, axis=1)\n    col_sums = np.sum(img_gray, axis=0)\n    rows = np.where(np.logical_or(row_sums < img.shape[1] * percentage,\n                                  row_sums > img.shape[1] * upper_percentage))[0]\n    cols = np.where(np.logical_or(col_sums < img.shape[0] * percentage,\n                                  col_sums > img.shape[0] * upper_percentage))[0]\n    im_crop = np.delete(img, rows, axis=0)\n    im_crop = np.delete(im_crop, cols, axis=1)\n    return im_crop*255","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.417007Z","iopub.execute_input":"2023-11-02T17:30:30.417384Z","iopub.status.idle":"2023-11-02T17:30:30.428050Z","shell.execute_reply.started":"2023-11-02T17:30:30.417345Z","shell.execute_reply":"2023-11-02T17:30:30.427070Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"class UBCDataset(Dataset):\n    def __init__(self, df, transforms=None):\n        self.df = df\n        self.file_names = df['file_path'].values\n        print(df['label'].values)\n        self.labels = df['label'].values\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path = self.file_names[index]\n        img = plt.imread(img_path)\n        img = trim(img)\n        label = self.labels[index]\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n            \n        return torch.tensor(img), torch.tensor(label, dtype=torch.long)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.431391Z","iopub.execute_input":"2023-11-02T17:30:30.431725Z","iopub.status.idle":"2023-11-02T17:30:30.442651Z","shell.execute_reply.started":"2023-11-02T17:30:30.431700Z","shell.execute_reply":"2023-11-02T17:30:30.441685Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"data_transforms = {\n    \"valid\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.)\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.443841Z","iopub.execute_input":"2023-11-02T17:30:30.444195Z","iopub.status.idle":"2023-11-02T17:30:30.452908Z","shell.execute_reply.started":"2023-11-02T17:30:30.444167Z","shell.execute_reply":"2023-11-02T17:30:30.452150Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"def get_train_file_path(image_id):\n    if os.path.exists(f\"{TRAIN_DIR}/{image_id}_thumbnail.png\"):\n        return f\"{TRAIN_DIR}/{image_id}_thumbnail.png\"\n    else:\n        return f\"{ALT_TRAIN_DIR}/{image_id}.png\"","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.454186Z","iopub.execute_input":"2023-11-02T17:30:30.454452Z","iopub.status.idle":"2023-11-02T17:30:30.464864Z","shell.execute_reply.started":"2023-11-02T17:30:30.454428Z","shell.execute_reply":"2023-11-02T17:30:30.463996Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"traindf = pd.read_csv('/kaggle/input/UBC-OCEAN/train.csv')\ntraindf['file_path'] = traindf['image_id'].apply(get_train_file_path)\n\n\n# # Step 1: Group by 'label'\n# grouped = traindf.groupby('label')\n\n# # Create a 5x3 subplot grid (5 rows and 3 columns)\n# fig, axes = plt.subplots(5, 3, figsize=(25, 25))\n\n# # Step 2 and 3: Select 3 random rows from each group and plot the images\n# for i, (label, group) in enumerate(grouped):\n#     # Select 3 random rows from the group\n#     random_rows = group.sample(3)\n    \n#     for j, (_, row) in enumerate(random_rows.iterrows()):\n#         ax = axes[i, j]  # Select the current subplot\n#         ax.set_title(f\"Label {label} | ID: {row['image_id']}\")  # Set the subplot title\n\n#         # Load and display the image from 'file_path'\n#         img = plt.imread(row['file_path'])\n#         img = (trim(img)).astype(np.uint8)\n#         ax.imshow(Image.fromarray(img))\n#         ax.axis('off')\n\n# # Hide any empty subplots\n# for i in range(len(grouped), 5 * 3):\n#     axes[i // 3, i % 3].axis('off')\n\n# plt.tight_layout()\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.466036Z","iopub.execute_input":"2023-11-02T17:30:30.466330Z","iopub.status.idle":"2023-11-02T17:30:30.747518Z","shell.execute_reply.started":"2023-11-02T17:30:30.466306Z","shell.execute_reply":"2023-11-02T17:30:30.746618Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"## Preparing the training data frame for training","metadata":{}},{"cell_type":"code","source":"traindf['label'][traindf['label']==\"HGSC\"] = 0\ntraindf['label'][traindf['label']==\"EC\"] = 1\ntraindf['label'][traindf['label']==\"CC\"] = 2\ntraindf['label'][traindf['label']==\"LGSC\"] = 3\ntraindf['label'][traindf['label']==\"MC\"] = 4\ntraindf","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.748693Z","iopub.execute_input":"2023-11-02T17:30:30.749006Z","iopub.status.idle":"2023-11-02T17:30:30.774591Z","shell.execute_reply.started":"2023-11-02T17:30:30.748969Z","shell.execute_reply":"2023-11-02T17:30:30.773547Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"     image_id label  image_width  image_height  is_tma  \\\n0           4     0        23785         20008   False   \n1          66     3        48871         48195   False   \n2          91     0         3388          3388    True   \n3         281     3        42309         15545   False   \n4         286     1        37204         30020   False   \n..        ...   ...          ...           ...     ...   \n533     65022     3        53355         46675   False   \n534     65094     4        55042         45080   False   \n535     65300     0        75860         27503   False   \n536     65371     0        42551         41800   False   \n537     65533     0        45190         33980   False   \n\n                                             file_path  \n0    /kaggle/input/UBC-OCEAN/train_thumbnails/4_thu...  \n1    /kaggle/input/UBC-OCEAN/train_thumbnails/66_th...  \n2          /kaggle/input/UBC-OCEAN/train_images/91.png  \n3    /kaggle/input/UBC-OCEAN/train_thumbnails/281_t...  \n4    /kaggle/input/UBC-OCEAN/train_thumbnails/286_t...  \n..                                                 ...  \n533  /kaggle/input/UBC-OCEAN/train_thumbnails/65022...  \n534  /kaggle/input/UBC-OCEAN/train_thumbnails/65094...  \n535  /kaggle/input/UBC-OCEAN/train_thumbnails/65300...  \n536  /kaggle/input/UBC-OCEAN/train_thumbnails/65371...  \n537  /kaggle/input/UBC-OCEAN/train_thumbnails/65533...  \n\n[538 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>label</th>\n      <th>image_width</th>\n      <th>image_height</th>\n      <th>is_tma</th>\n      <th>file_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>0</td>\n      <td>23785</td>\n      <td>20008</td>\n      <td>False</td>\n      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/4_thu...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>66</td>\n      <td>3</td>\n      <td>48871</td>\n      <td>48195</td>\n      <td>False</td>\n      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/66_th...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>91</td>\n      <td>0</td>\n      <td>3388</td>\n      <td>3388</td>\n      <td>True</td>\n      <td>/kaggle/input/UBC-OCEAN/train_images/91.png</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>281</td>\n      <td>3</td>\n      <td>42309</td>\n      <td>15545</td>\n      <td>False</td>\n      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/281_t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>286</td>\n      <td>1</td>\n      <td>37204</td>\n      <td>30020</td>\n      <td>False</td>\n      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/286_t...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>533</th>\n      <td>65022</td>\n      <td>3</td>\n      <td>53355</td>\n      <td>46675</td>\n      <td>False</td>\n      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/65022...</td>\n    </tr>\n    <tr>\n      <th>534</th>\n      <td>65094</td>\n      <td>4</td>\n      <td>55042</td>\n      <td>45080</td>\n      <td>False</td>\n      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/65094...</td>\n    </tr>\n    <tr>\n      <th>535</th>\n      <td>65300</td>\n      <td>0</td>\n      <td>75860</td>\n      <td>27503</td>\n      <td>False</td>\n      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/65300...</td>\n    </tr>\n    <tr>\n      <th>536</th>\n      <td>65371</td>\n      <td>0</td>\n      <td>42551</td>\n      <td>41800</td>\n      <td>False</td>\n      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/65371...</td>\n    </tr>\n    <tr>\n      <th>537</th>\n      <td>65533</td>\n      <td>0</td>\n      <td>45190</td>\n      <td>33980</td>\n      <td>False</td>\n      <td>/kaggle/input/UBC-OCEAN/train_thumbnails/65533...</td>\n    </tr>\n  </tbody>\n</table>\n<p>538 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = UBCDataset(traindf, transforms=data_transforms[\"valid\"]) # TODO: Add training transforms\ntrain_dataloader = DataLoader(train_dataset, batch_size=CONFIG['valid_batch_size'], \n                          num_workers=CONFIG[\"num_workers\"], shuffle=True, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.775759Z","iopub.execute_input":"2023-11-02T17:30:30.776076Z","iopub.status.idle":"2023-11-02T17:30:30.786395Z","shell.execute_reply.started":"2023-11-02T17:30:30.776051Z","shell.execute_reply":"2023-11-02T17:30:30.785141Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"[0 3 0 3 1 0 0 0 0 0 0 0 0 0 2 0 0 0 2 2 0 4 3 1 0 1 0 4 3 0 2 1 0 1 3 0 4\n 4 0 1 4 2 2 0 0 0 0 0 1 0 4 0 3 2 0 0 0 1 2 1 2 0 4 1 0 3 0 1 1 2 1 2 3 0\n 0 2 0 1 0 0 3 0 3 4 4 0 0 2 0 1 0 1 3 1 2 0 1 0 2 0 1 3 0 0 2 3 2 1 1 0 3\n 2 3 2 0 2 0 4 1 1 4 0 1 0 1 0 0 2 0 2 2 3 1 1 0 1 0 1 0 2 0 2 0 2 0 2 0 0\n 1 0 1 0 2 1 0 0 0 1 1 2 0 0 0 1 0 0 0 1 2 0 0 4 0 0 0 1 4 1 3 3 1 0 4 1 2\n 2 1 2 2 2 0 1 1 4 1 3 1 0 2 0 0 0 3 4 0 1 0 2 2 3 0 0 2 2 0 0 3 1 2 0 0 2\n 0 0 1 0 0 1 0 0 0 4 2 1 0 0 3 0 0 0 0 2 0 0 2 2 2 2 0 1 0 1 0 1 0 4 4 2 4\n 3 0 1 0 1 3 2 0 1 0 0 1 0 1 3 3 3 1 0 3 4 0 3 1 0 1 0 2 0 4 0 0 1 4 1 1 0\n 0 2 0 2 0 0 3 0 4 4 4 4 0 3 0 0 4 0 0 0 1 2 3 1 0 3 1 2 2 3 1 3 0 0 0 4 4\n 0 3 1 0 0 0 3 0 4 2 0 0 1 1 2 1 0 3 2 3 0 1 2 0 2 4 2 2 2 1 0 0 0 2 0 0 1\n 1 2 0 4 2 2 1 1 1 2 0 0 1 1 1 2 2 0 0 0 1 0 2 1 0 2 0 4 0 1 1 4 0 0 2 0 4\n 2 4 1 0 0 0 4 3 1 1 1 1 1 0 0 0 0 1 0 1 1 1 2 1 1 0 4 1 2 0 0 2 2 0 2 3 1\n 0 1 1 2 0 1 2 1 1 0 1 1 0 1 3 0 0 0 2 0 0 1 0 2 1 4 0 4 3 4 0 2 1 2 4 0 3\n 4 0 2 2 1 0 2 2 1 0 0 2 0 1 1 0 0 2 1 1 2 0 0 3 0 1 0 0 0 0 1 0 2 4 0 2 2\n 1 2 2 0 1 1 1 0 0 0 0 0 1 2 0 3 4 0 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"def visualize_transformed_image(dataframe, transform):\n    # Select a random row from the DataFrame\n    random_row = dataframe.sample(n=1).iloc[0]\n\n    # Get the image file path from the selected row\n    file_path = random_row['file_path']\n    print(random_row['is_tma'])\n    print(file_path)\n    # Load the original image\n    original_image = plt.imread(file_path)\n    trimmed_image = trim(original_image)\n    \n    # Apply the transform to the original image\n    transformed_image = transform(image=trimmed_image)[\"image\"]\n\n    # Visualize the original and transformed images\n    fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n\n    axes[0].set_title('Original Image')\n    axes[0].imshow(original_image)\n    axes[0].axis('off')\n\n    axes[1].set_title('Transformed Image')\n    axes[1].imshow(transformed_image.permute(1, 2, 0))\n    axes[1].axis('off')\n\n    plt.show()\n\n# Visualize a random image from the DataFrame with the transform applied\nif CONFIG[\"sandbox\"]:\n    visualize_transformed_image(traindf, data_transforms[\"valid\"])\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.787770Z","iopub.execute_input":"2023-11-02T17:30:30.788208Z","iopub.status.idle":"2023-11-02T17:30:30.799481Z","shell.execute_reply.started":"2023-11-02T17:30:30.788176Z","shell.execute_reply":"2023-11-02T17:30:30.798369Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"## Training and validation split from the training set","metadata":{}},{"cell_type":"code","source":"# Split the data into training and validation sets\nif CONFIG[\"sandbox\"]:\n    train_data, val_data = train_test_split(traindf, test_size=CONFIG[\"split_ratio\"], random_state=CONFIG[\"seed\"])\n\n    train_dataset = UBCDataset(train_data, transforms=data_transforms[\"valid\"]) # TODO: Add training transforms\n    train_dataloader = DataLoader(train_dataset, batch_size=CONFIG['valid_batch_size'], \n                              num_workers=CONFIG[\"num_workers\"], shuffle=True, pin_memory=True)\n\n    val_dataset = UBCDataset(val_data, transforms=data_transforms[\"valid\"]) \n    val_dataloader = DataLoader(val_dataset, batch_size=CONFIG['valid_batch_size'], \n                              num_workers=CONFIG[\"num_workers\"], shuffle=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.800570Z","iopub.execute_input":"2023-11-02T17:30:30.800866Z","iopub.status.idle":"2023-11-02T17:30:30.814499Z","shell.execute_reply.started":"2023-11-02T17:30:30.800842Z","shell.execute_reply":"2023-11-02T17:30:30.813457Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"# Train and save model","metadata":{}},{"cell_type":"markdown","source":"## Save model","metadata":{}},{"cell_type":"code","source":"def save_model(model: torch.nn.Module,\n               target_dir: str,\n               model_name: str):\n    \"\"\"Saves a PyTorch model to a target directory.\n    \n    Args:\n        model: A target PyTorch model to save.\n        target_dir: A directory for saving the model to.\n        model_name: A filename for the saved model. Should include either \".pth\" or \".pt\" as the file extension.\n        \n    Example usage:\n        save_model(model=model_0,\n        targer_dir=\"models\", \n        model_name=\"model_1\")\n    \"\"\"\n\n    # Create target directory\n    target_dir_path = Path(target_dir)\n    target_dir_path.mkdir(parents=True,\n                          exist_ok=True)\n\n    # Create model save path\n    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"),  \"model_name should end with '.pt' or '.pth'\"\n    model_save_path = target_dir_path / model_name\n\n    # Save the model state_dict()\n    print(f\"[INFO] Saving model to : {model_save_path}\")\n    torch.save(obj=model,\n               f=model_save_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.819614Z","iopub.execute_input":"2023-11-02T17:30:30.820156Z","iopub.status.idle":"2023-11-02T17:30:30.826724Z","shell.execute_reply.started":"2023-11-02T17:30:30.820123Z","shell.execute_reply":"2023-11-02T17:30:30.825824Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"## Importing a pretrained ViT\nThis is done online before we turn off the internet access. ","metadata":{}},{"cell_type":"code","source":"if CONFIG[\"train\"] or CONFIG[\"sandbox\"]:\n    # 1. Get pretrained weights for ViT-base\n    pretrained_weights = torchvision.models.Swin_V2_B_Weights.DEFAULT\n    #pretrained_vit_weights=torch.load('/kaggle/input/vit-weights/vit_b_16-c867db91.pth')\n    # 2. Setup a ViT model instance with pretrained weights\n    pretrained_model = torchvision.models.swin_v2_b(weights=pretrained_weights).to(CONFIG[\"device\"])\n\n    # 3. Freeze the base parameters\n    for parameter in pretrained_model.parameters():\n        parameter.requires_grad = False\n        parameter.to(CONFIG[\"device\"])\n\n    # 4. Change the classifier head\n    set_seed()\n    pretrained_model.head = nn.Sequential(\n            nn.Linear(in_features=pretrained_model.head.in_features, out_features = 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(in_features=256, out_features=64),\n            nn.ReLU(),\n            nn.Dropout(0.15),\n            nn.Linear(in_features=64, out_features = CONFIG[\"num_classes\"])\n            \n    ).to(CONFIG[\"device\"])\n    pretrained_model\n\n    save_model(pretrained_model, # Needs to be saved as a dataset so that it works offline\n             \"/kaggle/working/\",\n             f\"{CONFIG['model_name']}-pretrained.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.827841Z","iopub.execute_input":"2023-11-02T17:30:30.828174Z","iopub.status.idle":"2023-11-02T17:30:30.842473Z","shell.execute_reply.started":"2023-11-02T17:30:30.828138Z","shell.execute_reply":"2023-11-02T17:30:30.841601Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"\"\"\"\nContains functions for training and testing a PyTorch model.\n\"\"\"\nimport torch\n\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -> Tuple[float, float]:\n    \"\"\"Trains a PyTorch model for a single epoch.\n\n    Turns a target PyTorch model to training mode and then\n    runs through all of the required training steps (forward\n    pass, loss calculation, optimizer step).\n\n    Args:\n    model: A PyTorch model to be trained.\n    dataloader: A DataLoader instance for the model to be trained on.\n    loss_fn: A PyTorch loss function to minimize.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n    Returns:\n    A tuple of training loss and training accuracy metrics.\n    In the form (train_loss, train_accuracy). For example:\n\n    (0.1112, 0.8743)\n    \"\"\"\n    # Put model in train mode\n    model.train()\n\n    # Setup train loss and train accuracy values\n    train_loss, train_acc = 0, 0\n\n    # Loop through data loader data batches\n    for batch, (X, y) in enumerate(dataloader):\n        # Send data to target device\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate  and accumulate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item() \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Calculate and accumulate accuracy metric across all batches\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n    # Adjust metrics to get average loss and accuracy per batch \n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -> Tuple[float, float]:\n    \"\"\"Tests a PyTorch model for a single epoch.\n\n    Turns a target PyTorch model to \"eval\" mode and then performs\n    a forward pass on a testing dataset.\n\n    Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n    Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n\n    (0.0223, 0.8985)\n    \"\"\"\n    # Put model in eval mode\n    model.eval() \n\n    # Setup test loss and test accuracy values\n    test_loss, test_acc, balanced_acc = 0, 0, 0\n    pred_labels = []\n    true_labels = []\n    # Turn on inference context manager\n    with torch.inference_mode():\n        # Loop through DataLoader batches\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to target device\n            X, y = X.to(device), y.to(device)\n\n            # 1. Forward pass\n            test_pred_logits = model(X)\n\n            # 2. Calculate and accumulate loss\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n\n            # Calculate and accumulate accuracy\n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n            pred_labels = pred_labels + test_pred_labels.cpu().tolist()\n            true_labels = true_labels + y.cpu().tolist()\n\n    # Adjust metrics to get average loss and accuracy per batch \n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    balanced_acc = balanced_accuracy_score(true_labels, pred_labels)\n    return test_loss, test_acc, balanced_acc\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -> Dict[str, List]:\n    \"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Args:\n    model: A PyTorch model to be trained and tested.\n    train_dataloader: A DataLoader instance for the model to be trained on.\n    test_dataloader: A DataLoader instance for the model to be tested on.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n    epochs: An integer indicating how many epochs to train for.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n    Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for \n    each epoch.\n    In the form: {train_loss: [...],\n                  train_acc: [...],\n                  test_loss: [...],\n                  test_acc: [...]} \n    For example if training for epochs=2: \n                 {train_loss: [2.0616, 1.0537],\n                  train_acc: [0.3945, 0.3945],\n                  test_loss: [1.2641, 1.5706],\n                  test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n                \"train_acc\": [],\n                \"test_loss\": [],\n                \"test_acc\": [],\n                \"balanced_acc\": [],\n                }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n        test_loss, test_acc, balanced_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n        # Print out what's happening\n        print(\n                f\"Epoch: {epoch+1} | \"\n                f\"train_loss: {train_loss:.4f} | \"\n                f\"train_acc: {train_acc:.4f} | \"\n                f\"test_loss: {test_loss:.4f} | \"\n                f\"test_acc: {test_acc:.4f} | \"\n                f\"balanced_acc: {balanced_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n        results[\"balanced_acc\"].append(balanced_acc)\n\n    # Return the filled results at the end of the epochs\n    return results","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.843636Z","iopub.execute_input":"2023-11-02T17:30:30.843910Z","iopub.status.idle":"2023-11-02T17:30:30.871482Z","shell.execute_reply.started":"2023-11-02T17:30:30.843886Z","shell.execute_reply":"2023-11-02T17:30:30.870428Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# Create optimizer and loss function\ndef train_model(train_dl, test_dl, model_name, store_model=True):\n    optimizer = torch.optim.Adam(params=pretrained_model.parameters(),\n                                 lr=1e-3)\n    loss_fn = torch.nn.CrossEntropyLoss()\n    \n    if test_dl is None:\n        test_dl = train_dl\n    # Train the classifier head of the pretrained ViT feature extractor model\n    set_seed()\n    results = train(model=pretrained_model,\n                   train_dataloader=train_dl,\n                   test_dataloader=test_dl,\n                   optimizer=optimizer,\n                   loss_fn=loss_fn,\n                   epochs=CONFIG[\"epochs\"],\n                   device=CONFIG[\"device\"])\n    if store_model:\n        save_model(pretrained_model, # Needs to be saved as a dataset so that it works offline\n                 \"/kaggle/working/\",\n                 model_name)\n        \n    return results","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.872682Z","iopub.execute_input":"2023-11-02T17:30:30.873006Z","iopub.status.idle":"2023-11-02T17:30:30.886698Z","shell.execute_reply.started":"2023-11-02T17:30:30.872979Z","shell.execute_reply":"2023-11-02T17:30:30.885693Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"if CONFIG[\"train\"]:\n    results = train_model(train_dl=train_dataloader, test_dl=train_dataloader, model_name=f\"{CONFIG['model_name']}.pth\")\nelif CONFIG[\"sandbox\"]:\n    results = train_model(train_dl=train_dataloader, test_dl=val_dataloader, model_name=f\"{CONFIG['model_name']}.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.888046Z","iopub.execute_input":"2023-11-02T17:30:30.888453Z","iopub.status.idle":"2023-11-02T17:30:30.899062Z","shell.execute_reply.started":"2023-11-02T17:30:30.888406Z","shell.execute_reply":"2023-11-02T17:30:30.898023Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"def plot_loss_curves(results: Dict[str, List[float]]):\n    \"\"\"Plots training curves of a results dictionary.\n\n    Args:\n        results (dict): dictionary containing list of values, e.g.\n            {\"train_loss\": [...],\n             \"train_acc\": [...],\n             \"test_loss\": [...],\n             \"test_acc\": [...]}\n    \"\"\"\n\n    # Get the loss values of the results dictionary (training and test)\n    loss = results['train_loss']\n    test_loss = results['test_loss']\n\n    # Get the accuracy values of the results dictionary (training and test)\n    accuracy = results['train_acc']\n    test_accuracy = results['test_acc']\n\n    # Figure out how many epochs there were\n    epochs = range(len(results['train_loss']))\n\n    # Setup a plot\n    plt.figure(figsize=(15, 7))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label='train_loss')\n    plt.plot(epochs, test_loss, label='test_loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, label='train_accuracy')\n    plt.plot(epochs, test_accuracy, label='test_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend()\n    plt.show()\n    \n    # Plot accuracy\n    plt.subplot(1, 1, 1)\n    plt.plot(epochs, results[\"balanced_acc\"], label='balanced_acc')\n    plt.title('Balanced Accuracy for validation data')\n    plt.xlabel('Epochs')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.900339Z","iopub.execute_input":"2023-11-02T17:30:30.900690Z","iopub.status.idle":"2023-11-02T17:30:30.915512Z","shell.execute_reply.started":"2023-11-02T17:30:30.900656Z","shell.execute_reply":"2023-11-02T17:30:30.914650Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"# Plot the loss curves\nif CONFIG[\"train\"] or CONFIG[\"sandbox\"]:\n    plot_loss_curves(results) ","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.916469Z","iopub.execute_input":"2023-11-02T17:30:30.916748Z","iopub.status.idle":"2023-11-02T17:30:30.926500Z","shell.execute_reply.started":"2023-11-02T17:30:30.916724Z","shell.execute_reply":"2023-11-02T17:30:30.925478Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"## Testing the model","metadata":{}},{"cell_type":"code","source":"if CONFIG[\"train\"] or CONFIG[\"sandbox\"]:\n    model = torch.load(f\"/kaggle/working/{CONFIG['model_name']}.pth\")\nelse:\n    model = torch.load(f\"/kaggle/input/{CONFIG['model_name']}-trained/{CONFIG['model_name']}-trained.pth\") # Needs to be loaded from the input folder so that it works offline","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:30.927657Z","iopub.execute_input":"2023-11-02T17:30:30.927922Z","iopub.status.idle":"2023-11-02T17:30:31.211232Z","shell.execute_reply.started":"2023-11-02T17:30:30.927894Z","shell.execute_reply":"2023-11-02T17:30:31.210113Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"test_dataset = UBCDataset(df, transforms=data_transforms[\"valid\"])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'], \n                          num_workers=CONFIG[\"num_workers\"], shuffle=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:31.212825Z","iopub.execute_input":"2023-11-02T17:30:31.213158Z","iopub.status.idle":"2023-11-02T17:30:31.219599Z","shell.execute_reply.started":"2023-11-02T17:30:31.213132Z","shell.execute_reply":"2023-11-02T17:30:31.218592Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"[0]\n","output_type":"stream"}]},{"cell_type":"code","source":"class_names = [\"HGSC\",\"EC\",\"CC\",\"LGSC\",\"MC\"]\n\n# Function to load the model and predict on selected image\ndef predict_on_image(model, image_path, device=CONFIG[\"device\"]):\n    print(device)\n    # Load the image and turn it into torch.float32 (same type as model)\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    # Preprocess the image to get it between 0 and 1\n    transform = data_transforms[\"valid\"]\n    \n    image = transform(image=img)[\"image\"] # make sure image has batch dimension (shape: [batch_size, height, width, color_channels])\n    image = torch.tensor(np.expand_dims(image, axis=0))\n    print(image.to(device).dtype)\n    \n    # Predict on image\n    model.eval()\n    with torch.inference_mode():\n        # Put image to target device\n        image = image.to(device)\n        print(image.device)\n        \n        # Get prediction logits\n        pred_logits = model(image)\n        print(pred_logits)\n        # Get prediction probabilities\n        pred_probs = torch.softmax(pred_logits, dim=1)\n\n        # Get prediction label\n        pred_label = torch.argmax(pred_probs, dim=1).detach().cpu()\n        pred_label_class = class_names[pred_label]\n\n    print(f\"[INFO] Pred label: {pred_label} Pred class: {pred_label_class}, Pred prob: {pred_probs.max():.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:31.220815Z","iopub.execute_input":"2023-11-02T17:30:31.221135Z","iopub.status.idle":"2023-11-02T17:30:31.231998Z","shell.execute_reply.started":"2023-11-02T17:30:31.221085Z","shell.execute_reply":"2023-11-02T17:30:31.231085Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"CONFIG[\"device\"]\nclass_names[int(traindf['label'][traindf['image_id']==10077])]","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:31.233226Z","iopub.execute_input":"2023-11-02T17:30:31.234388Z","iopub.status.idle":"2023-11-02T17:30:31.245383Z","shell.execute_reply.started":"2023-11-02T17:30:31.234342Z","shell.execute_reply":"2023-11-02T17:30:31.244594Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"'EC'"},"metadata":{}}]},{"cell_type":"code","source":"if CONFIG[\"train\"]:\n    predict_on_image(model, \"/kaggle/input/UBC-OCEAN/train_thumbnails/10077_thumbnail.png\", device=CONFIG[\"device\"])","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:31.246443Z","iopub.execute_input":"2023-11-02T17:30:31.246704Z","iopub.status.idle":"2023-11-02T17:30:31.253875Z","shell.execute_reply.started":"2023-11-02T17:30:31.246682Z","shell.execute_reply":"2023-11-02T17:30:31.252924Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"# Plot images and make predictions for manual testing\ndef show_images_from_dataframe(dataframe, num_images):\n    # Select 'num_images' random rows from the DataFrame\n    selected_rows = dataframe.sample(num_images)\n    \n    for _, row in selected_rows.iterrows():\n        file_path = row['file_path']\n        image = cv2.imread(file_path)\n        \n        plt.figure(figsize=(30, 30))\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        plt.title(f\"Image ID: {row['image_id']}\")\n        plt.axis('off')\n        plt.show()\n    return selected_rows\n\nif CONFIG[\"sandbox\"]:        \n    selected_rows = show_images_from_dataframe(val_data, 5)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:31.254930Z","iopub.execute_input":"2023-11-02T17:30:31.255243Z","iopub.status.idle":"2023-11-02T17:30:31.268464Z","shell.execute_reply.started":"2023-11-02T17:30:31.255219Z","shell.execute_reply":"2023-11-02T17:30:31.267557Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"def predict_and_visualize_images_with_labels(selected_rows, model):\n    model.eval()\n\n    for _, row in selected_rows.iterrows():\n        file_path = row['file_path']\n        image = cv2.imread(file_path)\n\n        # Apply the transform to the original image\n        transformed_image = data_transforms[\"valid\"](image=image)[\"image\"].unsqueeze(0).to(CONFIG[\"device\"])\n\n        # Perform inference\n        with torch.no_grad():\n            output = model(transformed_image)\n\n        # Get the predicted class label\n        predicted_class = torch.argmax(output).item()\n\n        plt.figure(figsize=(8, 8))\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        plt.title(f\"Image ID: {row['image_id']}\\nTrue Label: {class_names[row['label']]}\\nPredicted Label: {class_names[predicted_class]}\")\n        plt.axis('off')\n        plt.show()\n        \nif CONFIG[\"sandbox\"]:       \n    predict_and_visualize_images_with_labels(selected_rows, model)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:31.269686Z","iopub.execute_input":"2023-11-02T17:30:31.270325Z","iopub.status.idle":"2023-11-02T17:30:31.280659Z","shell.execute_reply.started":"2023-11-02T17:30:31.270300Z","shell.execute_reply":"2023-11-02T17:30:31.279746Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"def get_label(label):\n    return class_names[label]","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:31.281965Z","iopub.execute_input":"2023-11-02T17:30:31.283135Z","iopub.status.idle":"2023-11-02T17:30:31.292286Z","shell.execute_reply.started":"2023-11-02T17:30:31.283077Z","shell.execute_reply":"2023-11-02T17:30:31.290981Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nif CONFIG[\"sandbox\"]:\n    preds = []\n    true = []\n    with torch.no_grad():\n        bar = tqdm(enumerate(val_dataloader), total=len(val_dataloader))\n        for step, (data, y) in bar:        \n            images = data.to(CONFIG[\"device\"], dtype=torch.float)        \n            batch_size = images.size(0)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            true.append(y.detach().cpu().numpy())\n            preds.append(predicted.detach().cpu().numpy())\n    preds = [get_label(item) for item in np.concatenate(preds).flatten()]\n    true = [get_label(item) for item in np.concatenate(true).flatten()]\n\n    # Compute the confusion matrix\n    cm = confusion_matrix(true, preds)\n\n    # Create a ConfusionMatrixDisplay with display_labels parameter\n    cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n    cmd.plot(cmap=plt.cm.Blues)  # You can choose a different color map if needed\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:31.293602Z","iopub.execute_input":"2023-11-02T17:30:31.293916Z","iopub.status.idle":"2023-11-02T17:30:31.304829Z","shell.execute_reply.started":"2023-11-02T17:30:31.293882Z","shell.execute_reply":"2023-11-02T17:30:31.303841Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"# Create submission","metadata":{}},{"cell_type":"code","source":"preds = []\nwith torch.no_grad():\n    bar = tqdm(enumerate(test_loader), total=len(test_loader))\n    for step, (data, _) in bar:        \n        images = data.to(CONFIG[\"device\"], dtype=torch.float)        \n        batch_size = images.size(0)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        \n        preds.append( predicted.detach().cpu().numpy() )\npreds = np.concatenate(preds).flatten()\npreds\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:31.306108Z","iopub.execute_input":"2023-11-02T17:30:31.306457Z","iopub.status.idle":"2023-11-02T17:30:32.160274Z","shell.execute_reply.started":"2023-11-02T17:30:31.306431Z","shell.execute_reply":"2023-11-02T17:30:32.159183Z"},"trusted":true},"execution_count":91,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a62eabd1218471b9235988329576af2"}},"metadata":{}},{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"array([0])"},"metadata":{}}]},{"cell_type":"code","source":"df_sub_2 = pd.read_csv(f\"{ROOT_DIR}/test.csv\")\ndf_sub_2","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:32.161593Z","iopub.execute_input":"2023-11-02T17:30:32.161908Z","iopub.status.idle":"2023-11-02T17:30:32.177162Z","shell.execute_reply.started":"2023-11-02T17:30:32.161878Z","shell.execute_reply":"2023-11-02T17:30:32.176163Z"},"trusted":true},"execution_count":92,"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"   image_id  image_width  image_height\n0        41        28469         16987","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>image_width</th>\n      <th>image_height</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41</td>\n      <td>28469</td>\n      <td>16987</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_sub_2 = pd.read_csv(f\"{ROOT_DIR}/test.csv\")\ndf_sub_2[\"label\"] = preds    \ndf_sub_2[\"label\"] = df_sub_2[\"label\"].apply(get_label)\ndf_sub_2 = df_sub_2.drop(\"image_width\", axis=1)\ndf_sub_2 = df_sub_2.drop(\"image_height\", axis=1)\ndf_sub_2.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:32.178356Z","iopub.execute_input":"2023-11-02T17:30:32.178633Z","iopub.status.idle":"2023-11-02T17:30:32.190436Z","shell.execute_reply.started":"2023-11-02T17:30:32.178609Z","shell.execute_reply":"2023-11-02T17:30:32.189562Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"## Check that submission looks reasonable","metadata":{}},{"cell_type":"code","source":"df_sub_2","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:30:32.191508Z","iopub.execute_input":"2023-11-02T17:30:32.191743Z","iopub.status.idle":"2023-11-02T17:30:32.200375Z","shell.execute_reply.started":"2023-11-02T17:30:32.191722Z","shell.execute_reply":"2023-11-02T17:30:32.199335Z"},"trusted":true},"execution_count":94,"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"   image_id label\n0        41  HGSC","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41</td>\n      <td>HGSC</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}